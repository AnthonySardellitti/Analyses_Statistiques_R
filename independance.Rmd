---
output:
  html_document: default
  pdf_document: default
---

# Test ind√©pendance {#independance}

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
library(kableExtra)
library(dplyr)
taille <- 12
```


Les tests d'ind√©pendances permettent de d√©finir s'il existe un lien entre deux variables. Il existe diff√©rent test d'ind√©pence, en voici quelques exemples :

- Test ind√©pendance entre deux variables quantitatives / Test de corr√©lation Pearson
- Test d'ind√©pendance entre deux variables qualitatives / Test du Chi¬≤
- Test d'ind√©pendance entre une variable qualitative et une quantitative / Test de Fisher avec l'analyse de la variance (ANOVA)


## Test de corr√©lation

L'int√©r√™t des tests de corr√©lation est d'apporter plus de pertinence et fiabilit√© aux coefficients de corr√©lation. Il existe diff√©rents test de corr√©lation, nous utilisons celui de Pearson.

On travaille avec le jeu de donn√©es fromage üßÄ

```{r}
df <- read.csv(file = "./datasets/fromage.txt", sep = "\t", row.names = 1)
```

```{r}
plot(df)
```

```{r}
cor(df, method = "pearson")
```

```{r, warning=TRUE}
library(corrplot)
corrplot(cor(df, method = "pearson"))
```


On pose les hypoth√®ses de d√©part

H0 : Variables ind√©pendantes si p-value > 5%
<br> H1 : Variables non ind√©pendantes si p-value < 5%


### Lipide vs Magnesium

La premi√®re sortie correspond au coefficient de corr√©lation, la seconde √† la p-value (ou probabilit√© critique)

```{r}
cor(x = df$lipides, y = df$magnesium)
```

```{r}
cor.test(x = df$lipides, y = df$magnesium)
```

<br> H1 : Variables non ind√©pendantes si p-value < 5%


### Sodium vs Retinol

```{r}
cor.test(x = df$sodium, y = df$retinol)
```

H0 : Variables ind√©pendantes si p-value > 5%
<br>
Si on veut rejeter H0 et prendre H1, j'ai 45,5% de chance de me tromper

Les tests statistiques sont tr√©s sensibles √† la taille de l'√©chantillon. 
Un coefficient de corr√©lation de 0.14 n'aura pas la m√™me significativit√© sur un √©chantillon de 29 fromages qu'un √©chantillon de 319 fromages avec le m√™me coefficient de corr√©lation.

On construit un dataframe en dupliquant le nombre de lignes

```{r}
sodium <- rep(df$sodium,times = 10)
retinol <- rep(df$retinol,times = 10)
nom <- rep(rownames(df),times = 10)

df_10 <- data.frame(nom,sodium,retinol)

```

Chaque fromage appara√Æt plusieurs fois, on a augment√© la taille de l'√©chantillon


```{r}
table(df_10$nom)
```

On effectue un autre test de corr√©lation avec les m√™mes variables sur l'√©chantillon plus grand.

```{r}
cor.test(x = df_10$sodium, y = df_10$retinol)
```
H1 : Variables non ind√©pendantes si p-value < 5%

On obtient logiquement le m√™me coefficient de corr√©lation, mais en revanche, cette fois si la p-value est proche de 0.



### Matrice des p-values

On effectue un test de corr√©lation sur chaque variable 2 √† 2 en isolant uniquement la p-value

```{r}
get_pvalue <- function(x,y){
  p <- cor.test(df[,x],df[,y])$p.value
  return(p)
}

colonne <- colnames(df)
ligne <- colnames(df)
df_pvalues <- outer(X = colonne, Y = ligne, FUN = Vectorize(get_pvalue))
colnames(df_pvalues) <- colnames(df)
rownames(df_pvalues) <- colnames(df)

```

On affiche la matrice des corr√©lations avec un gradiant de couleur

```{r}
corrplot(df_pvalues,
         cl.lim=c(0,1), method="number", type="upper",
         col=colorRampPalette(c("white","white","red","yellow","green"))(20))
```


### Cas de relation non lin√©aire


[Plus d'infos ici](http://grasland.script.univ-paris-diderot.fr/STAT98/stat98_6/stat98_6.htm)


Cas d'une relation non-lin√©aire et non-monotone


```{r}
x <- -10:10
y <- x^2 + rnorm(n = length(x))

plot(x,y)
```

```{r}
cor.test(x, y, method = "pearson")
cor.test(x, y, method = "spearman")
cor.test(x, y, method = "kendall")
```


## Test du CHI¬≤

L'int√©r√™t du test du Khi¬≤ est de mesurer l'ind√©pendance entre deux variables qualitatives √† partir du tableau de contigence.

H0 : Variables ind√©pendantes si p-value > 5%
H1 : Variables non ind√©pendantes si p-value < 5%

### Titanic

On travaille sur le jeu de donn√©es Titanic üßä‚õ¥

```{r}
df <- read.csv(file = "./datasets/Titanic.csv", row.names = 1)
```

```{r}
df_count <- table(df$Survived, df$PClass)
```

```{r}
resultat <-chisq.test(df$Survived,df$PClass)
resultat
```

H1 : Variables non ind√©pendantes si p-value < 5%

```{r}
attributes(resultat)
resultat$expected
```


### Exemple du support

```{r}
data <- matrix(rbind(c(693,886,534,153),c(597,696,448,95)),ncol=4)
```

```{r}
chisq.test(data)
```

H0 : Variables ind√©pendantes si p-value > 5%
Si on veut rejeter H0 et prendre H1, j'ai 10,9% de chance de me tromper

Lecture dans la table du Chi2

```{r}
p <- seq(0.80, 0.90, 0.005)
dof <- seq(1,3)
chisq_table <- outer(p, dof, function(x,y) qchisq(x,y))
chisq_table <- t(chisq_table)
colnames(chisq_table) <- 1 - p
rownames(chisq_table) <- dof
chisq_table <- round(chisq_table,2)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
kable(chisq_table, "html") %>% kable_styling("striped", font_size = taille) %>% scroll_box(width = "100%", height = "100px")
```


üì¢ Taille de l'√©chantillon

Les tests d'ind√©pendance sont tr√©s sensibles √† la taille des √©chantillons. Ici on divise par 100 pour avoir des effectifs faibles mais en conservant les r√©partitions.


```{r, warning=FALSE, message=FALSE}
chisq.test(data/100)
```
H0 : Variables ind√©pendantes si p-value > 5%


Ici on multiplie par 100 pour avoir des effectifs grands mais en conservant les r√©partitions

```{r}
chisq.test(data*100)
```

H1 : Variables non ind√©pendantes si p-value < 5%


## ANOVA 1

On effectue une analyse de variance pour mesurer l'ind√©pendance entre une variable qualitative et une quantitative. 


Pour illustrer cela, on utilise le jeu de donn√©es Hotdogs üå≠.

```{r}
df <- read.csv(file = "./datasets/Hotdogs.csv", sep = ";")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
kable(df, "html") %>% kable_styling("striped", font_size = taille) %>% scroll_box(width = "100%", height = "100px")
```


On va tester l'ind√©pendance entre la variable qualitative `Type` et la variable quantitatives `Calories`. 

```{r}
boxplot(Calories  ~ Type, data = df, 
        horizontal = TRUE)
```

Dans une ANOVA, on cherche √† d√©terminer si les moyennes des groupes sont significativement diff√©rentes. On pose donc :

* H0 : Les moyennes de chaque groupe sont √©gales si p-value > 5%
* H1 : Les moyennes de chaque groupe ne sont pas toutes √©gales si p-value < 5%

Dans une ANOVA, on √©tudie la variance de chacun de ces groupes. Pour cela on utilise la fonction `aov()`.

```{r}
aov <- aov(formula = Calories ~ Type, data = df)
summary(aov)
```
H1 : Les moyennes de chaque groupe ne sont pas toutes √©gales


Quand on dispose d'un petit √©chantillon, la pertinence de ce test repose sur la validation de plusieurs hypoth√®ses :

* - l'ind√©pendance entre les √©chantillons de chaque groupe
* - l'√©galit√© des des variances que l'on peut verifier avec un test de Bartlett.
* - la normal

### L'ind√©pendance

L'ind√©pendance est une des 3 conditions de validit√© d'une ANOVA.
Seul le contexte de l'√©tude permet de s'assurer de l'ind√©pendance entre les √©chantillons de chaque groupe (ici *beef*, *poultry*, *chicken*.)

### L'√©galit√© des variances

On parle aussi d'homosc√©dasticit√©. C'est une des 3 conditions de validit√© d'une ANOVA. On cherche √† d√©montrer que les variances de chaque groupe sont √©gales. Dans un boxplot, l'amplitude des bo√Ætes traduit graphiquement l'√©galit√© des variances.

```{r}
boxplot(Calories  ~ Type, data = df, 
        horizontal = TRUE)
```

Mais c'est le test de bartlett qui permet de tester si les variances sont significativement diff√©rentes ou non avec :

* H0 : Les variances de chaque groupe sont √©gales si p-value > 5%
* H1 : Les variances de chaque groupe ne sont pas toutes √©gales < 5%

```{r}
bartlett.test(Calories  ~ Type, data = df)
```
H0 : Les variances de chaque groupe sont √©gales.
La deuxi√®me condition pour effectuer une anova est valid√©e.

### Normalit√© des r√©sidus

C'est une des 3 conditions de validit√© d'une ANOVA. L'objectif est de s'assurer que les r√©sidus suivent une loi normale afin de ne pas affirmer qu'il existe une diff√©rence de moyenne entre les groupes qui serait caus√©e par le hasard.

Dans R, on utilise le test de Shapiro-Wilk pour tester la normalit√© des r√©sidus o√π :

* H0 : Les r√©sidus suivent une loi normale si p-value > 5%
* H1 : Les r√©sidus ne suivent pas une loi normale si p-value < 5%

```{r}
aov <- aov(formula = Calories ~ Type, data = df)
shapiro.test(aov$residuals)
```
H1 : Les r√©sidus ne suivent pas une loi normale

### Calcul - Cas des variances √©gales

```{r}
a <- seq(from = 1, to = 11, length.out = 9   )
b <- seq(from = 31, to = 40, length.out = 9   )
c <- seq(from = 51, to = 62, length.out = 9   )
```

```{r}
df <- data.frame(Valeur = c(a,b,c), Groupe = c(rep("A",9),
                        rep("B",9),
                        rep("C",9)))
head(df)
```
```{r}
boxplot(Valeur  ~ Groupe, data = df, 
        col = 1:3, horizontal = TRUE)
```

Comment calculer le tableau r√©captitulatif de l'analyse de la variance :

```{r, echo=FALSE}
summary(aov(formula = Valeur ~ Groupe, data = df))
```

Variance intra classes

```{r}
SCE_a <- (a - mean(a))^2
SCE_b <- (b - mean(b))^2
SCE_c <- (c - mean(c))^2
intra <- sum(SCE_a + SCE_b + SCE_c)
intra
```

Variance inter classes

```{r}
moyenne <- mean(df$Valeur)
moyenne_facteur <- tapply(X = df$Valeur, 
                          INDEX = df$Groupe,
                          FUN = mean)

longueur_facteur <- tapply(X = df$Valeur, 
                           INDEX = df$Groupe,
                           FUN = length)
inter <- sum(longueur_facteur*((moyenne_facteur - moyenne)^2))
inter
```
Degr√© de libert√©

```{r}
n <- nrow(df)
p <- length(levels(df$Groupe))
dof_inter <- p - 1
dof_intra <- n - p
dof_inter
dof_intra
```

Calcul de la statistique de test de Fisher

```{r}
Stat_Fisher <- (inter/dof_inter) / (intra/dof_intra)
Stat_Fisher
```

On lit dans la table de Fisher

```{r}
pvalue <- 1-pf(q = Stat_Fisher,
   df1 = dof_inter,
   df2 = dof_intra)
pvalue
```

R√©ciproque de la loi de Fisher pour retrouver la statistique de test.

```{r}
qf(p = 1-pvalue, df1 = dof_inter, df2 = dof_intra)
```

### Calcul - Cas des variances in√©gales

```{r}
a <- seq(from = 1, to = 40, length.out = 9   )
b <- seq(from = 10, to = 30, length.out = 9   )
c <- seq(from = 25, to = 30, length.out = 9   )
```

```{r}
df <- data.frame(Valeur = c(a,b,c), Groupe = c(rep("A",9),
                        rep("B",9),
                        rep("C",9)))
head(df)
```
```{r}
boxplot(Valeur  ~ Groupe, data = df, 
        col = 1:3, horizontal = TRUE)
```

Comment calculer le tableau r√©captitulatif de l'analyse de la variance :

```{r, echo=FALSE}
summary(aov(formula = Valeur ~ Groupe, data = df))
```

Variance intra classes

```{r}
SCE_a <- (a - mean(a))^2
SCE_b <- (b - mean(b))^2
SCE_c <- (c - mean(c))^2
intra <- sum(SCE_a + SCE_b + SCE_c)
intra
```

Variance inter classes

```{r}
moyenne <- mean(df$Valeur)
moyenne_facteur <- tapply(X = df$Valeur, 
                          INDEX = df$Groupe,
                          FUN = mean)

longueur_facteur <- tapply(X = df$Valeur, 
                           INDEX = df$Groupe,
                           FUN = length)
inter <- sum(longueur_facteur*((moyenne_facteur - moyenne)^2))
inter
```
Degr√© de libert√©

```{r}
n <- nrow(df)
p <- length(levels(df$Groupe))
dof_inter <- p - 1
dof_intra <- n - p
dof_inter
dof_intra
```

Calcul de la statistique de test de Fisher

```{r}
Stat_Fisher <- (inter/dof_inter) / (intra/dof_intra)
Stat_Fisher
```

On lit dans la table de Fisher

```{r}
pvalue <- 1-pf(q = Stat_Fisher,
   df1 = dof_inter,
   df2 = dof_intra)
pvalue
```

R√©ciproque de la loi de Fisher pour retrouver la statistique de test.

```{r}
qf(p = 1-pvalue, df1 = dof_inter, df2 = dof_intra)
```

## ANOVA 2

