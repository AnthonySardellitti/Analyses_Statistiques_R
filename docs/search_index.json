[
["independance.html", "Chapitre 2 Test ind√©pendance 2.1 Test de corr√©lation 2.2 Test du CHI¬≤ 2.3 ANOVA 1 2.4 ANOVA 2", " Chapitre 2 Test ind√©pendance Les tests d‚Äôind√©pendances permettent de d√©finir s‚Äôil existe un lien entre deux variables. Il existe diff√©rent test d‚Äôind√©pence, en voici quelques exemples : Test ind√©pendance entre deux variables quantitatives / Test de corr√©lation Pearson Test d‚Äôind√©pendance entre deux variables qualitatives / Test du Chi¬≤ Test d‚Äôind√©pendance entre une variable qualitative et une quantitative / Test de Fisher avec l‚Äôanalyse de la variance (ANOVA) 2.1 Test de corr√©lation L‚Äôint√©r√™t des tests de corr√©lation est d‚Äôapporter plus de pertinence et fiabilit√© aux coefficients de corr√©lation. Il existe diff√©rents test de corr√©lation, nous utilisons celui de Pearson. On travaille avec le jeu de donn√©es fromage üßÄ df &lt;- read.csv(file = &quot;./datasets/fromage.txt&quot;, sep = &quot;\\t&quot;, row.names = 1) plot(df) cor(df, method = &quot;pearson&quot;) ## calories sodium calcium lipides retinol ## calories 1.00000000 0.447224073 0.433400087 0.98363377 -0.04288511 ## sodium 0.44722407 1.000000000 0.005958892 0.48334434 0.14432755 ## calcium 0.43340009 0.005958892 1.000000000 0.34113187 -0.28954677 ## lipides 0.98363377 0.483344342 0.341131874 1.00000000 -0.01835291 ## retinol -0.04288511 0.144327552 -0.289546767 -0.01835291 1.00000000 ## folates -0.32109786 0.136561416 -0.637502539 -0.27654282 0.51726643 ## proteines 0.88537159 0.275997906 0.610704864 0.80930257 -0.03917254 ## cholesterol 0.96192339 0.332993972 0.428446215 0.95544387 -0.08697243 ## magnesium 0.74544320 0.031112805 0.710480424 0.68986008 -0.09980065 ## folates proteines cholesterol magnesium ## calories -0.3210979 0.88537159 0.96192339 0.74544320 ## sodium 0.1365614 0.27599791 0.33299397 0.03111281 ## calcium -0.6375025 0.61070486 0.42844622 0.71048042 ## lipides -0.2765428 0.80930257 0.95544387 0.68986008 ## retinol 0.5172664 -0.03917254 -0.08697243 -0.09980065 ## folates 1.0000000 -0.35287125 -0.36634610 -0.44706739 ## proteines -0.3528712 1.00000000 0.81713835 0.78777726 ## cholesterol -0.3663461 0.81713835 1.00000000 0.75245338 ## magnesium -0.4470674 0.78777726 0.75245338 1.00000000 library(corrplot) ## Warning: package &#39;corrplot&#39; was built under R version 3.5.3 ## corrplot 0.84 loaded corrplot(cor(df, method = &quot;pearson&quot;)) On pose les hypoth√®ses de d√©part H0 : Variables ind√©pendantes si p-value &gt; 5% H1 : Variables non ind√©pendantes si p-value &lt; 5% 2.1.1 Lipide vs Magnesium La premi√®re sortie correspond au coefficient de corr√©lation, la seconde √† la p-value (ou probabilit√© critique) cor(x = df$lipides, y = df$magnesium) ## [1] 0.6898601 cor.test(x = df$lipides, y = df$magnesium) ## ## Pearson&#39;s product-moment correlation ## ## data: df$lipides and df$magnesium ## t = 4.9515, df = 27, p-value = 3.469e-05 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4327766 0.8431785 ## sample estimates: ## cor ## 0.6898601 H1 : Variables non ind√©pendantes si p-value &lt; 5% 2.1.2 Sodium vs Retinol cor.test(x = df$sodium, y = df$retinol) ## ## Pearson&#39;s product-moment correlation ## ## data: df$sodium and df$retinol ## t = 0.75788, df = 27, p-value = 0.4551 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2345870 0.4851693 ## sample estimates: ## cor ## 0.1443276 H0 : Variables ind√©pendantes si p-value &gt; 5% Si on veut rejeter H0 et prendre H1, j‚Äôai 45,5% de chance de me tromper Les tests statistiques sont tr√©s sensibles √† la taille de l‚Äô√©chantillon. Un coefficient de corr√©lation de 0.14 n‚Äôaura pas la m√™me significativit√© sur un √©chantillon de 29 fromages qu‚Äôun √©chantillon de 319 fromages avec le m√™me coefficient de corr√©lation. On construit un dataframe en dupliquant le nombre de lignes sodium &lt;- rep(df$sodium,times = 10) retinol &lt;- rep(df$retinol,times = 10) nom &lt;- rep(rownames(df),times = 10) df_10 &lt;- data.frame(nom,sodium,retinol) Chaque fromage appara√Æt plusieurs fois, on a augment√© la taille de l‚Äô√©chantillon table(df_10$nom) ## ## Babybel Beaufort Bleu ## 10 10 10 ## Camembert Cantal CarredelEst ## 10 10 10 ## Chabichou Chaource Cheddar ## 10 10 10 ## Comte Coulomniers Edam ## 10 10 10 ## Emmental Fr.chevrepatemolle Fr.fondu.45 ## 10 10 10 ## Fr.frais20nat. Fr.frais40nat. Maroilles ## 10 10 10 ## Morbier Parmesan Petitsuisse40 ## 10 10 10 ## PontlEveque Pyrenees Reblochon ## 10 10 10 ## Rocquefort SaintPaulin Tome ## 10 10 10 ## Vacherin Yaourtlaitent.nat. ## 10 10 On effectue un autre test de corr√©lation avec les m√™mes variables sur l‚Äô√©chantillon plus grand. cor.test(x = df_10$sodium, y = df_10$retinol) ## ## Pearson&#39;s product-moment correlation ## ## data: df_10$sodium and df_10$retinol ## t = 2.4752, df = 288, p-value = 0.01389 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.0296407 0.2552637 ## sample estimates: ## cor ## 0.1443276 H1 : Variables non ind√©pendantes si p-value &lt; 5% On obtient logiquement le m√™me coefficient de corr√©lation, mais en revanche, cette fois si la p-value est proche de 0. 2.1.3 Matrice des p-values On effectue un test de corr√©lation sur chaque variable 2 √† 2 en isolant uniquement la p-value get_pvalue &lt;- function(x,y){ p &lt;- cor.test(df[,x],df[,y])$p.value return(p) } colonne &lt;- colnames(df) ligne &lt;- colnames(df) df_pvalues &lt;- outer(X = colonne, Y = ligne, FUN = Vectorize(get_pvalue)) colnames(df_pvalues) &lt;- colnames(df) rownames(df_pvalues) &lt;- colnames(df) On affiche la matrice des corr√©lations avec un gradiant de couleur corrplot(df_pvalues, cl.lim=c(0,1), method=&quot;number&quot;, type=&quot;upper&quot;, col=colorRampPalette(c(&quot;white&quot;,&quot;white&quot;,&quot;red&quot;,&quot;yellow&quot;,&quot;green&quot;))(20)) 2.1.4 Cas de relation non lin√©aire Plus d‚Äôinfos ici Cas d‚Äôune relation non-lin√©aire et non-monotone x &lt;- -10:10 y &lt;- x^2 + rnorm(n = length(x)) plot(x,y) cor.test(x, y, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 0.0057615, df = 19, p-value = 0.9955 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.4306107 0.4327616 ## sample estimates: ## cor ## 0.001321772 cor.test(x, y, method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: x and y ## S = 1520, p-value = 0.9573 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.01298701 cor.test(x, y, method = &quot;kendall&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: x and y ## T = 107, p-value = 0.9287 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.01904762 2.2 Test du CHI¬≤ L‚Äôint√©r√™t du test du Khi¬≤ est de mesurer l‚Äôind√©pendance entre deux variables qualitatives √† partir du tableau de contigence. H0 : Variables ind√©pendantes si p-value &gt; 5% H1 : Variables non ind√©pendantes si p-value &lt; 5% 2.2.1 Titanic On travaille sur le jeu de donn√©es Titanic üßä‚õ¥ df &lt;- read.csv(file = &quot;./datasets/Titanic.csv&quot;, row.names = 1) df_count &lt;- table(df$Survived, df$PClass) resultat &lt;-chisq.test(df$Survived,df$PClass) resultat ## ## Pearson&#39;s Chi-squared test ## ## data: df$Survived and df$PClass ## X-squared = 172.3, df = 2, p-value &lt; 2.2e-16 H1 : Variables non ind√©pendantes si p-value &lt; 5% attributes(resultat) ## $names ## [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;method&quot; &quot;data.name&quot; &quot;observed&quot; ## [7] &quot;expected&quot; &quot;residuals&quot; &quot;stdres&quot; ## ## $class ## [1] &quot;htest&quot; resultat$expected ## df$PClass ## df$Survived 1st 2nd 3rd ## 0 211.642 184.03656 467.3214 ## 1 110.358 95.96344 243.6786 2.2.2 Exemple du support data &lt;- matrix(rbind(c(693,886,534,153),c(597,696,448,95)),ncol=4) chisq.test(data) ## ## Pearson&#39;s Chi-squared test ## ## data: data ## X-squared = 6.0504, df = 3, p-value = 0.1092 H0 : Variables ind√©pendantes si p-value &gt; 5% Si on veut rejeter H0 et prendre H1, j‚Äôai 10,9% de chance de me tromper Lecture dans la table du Chi2 p &lt;- seq(0.80, 0.90, 0.005) dof &lt;- seq(1,3) chisq_table &lt;- outer(p, dof, function(x,y) qchisq(x,y)) chisq_table &lt;- t(chisq_table) colnames(chisq_table) &lt;- 1 - p rownames(chisq_table) &lt;- dof chisq_table &lt;- round(chisq_table,2) 0.2 0.195 0.19 0.185 0.18 0.175 0.17 0.165 0.16 0.155 0.15 0.145 0.14 0.135 0.13 0.125 0.12 0.115 0.11 0.105 0.1 1.64 1.68 1.72 1.76 1.80 1.84 1.88 1.93 1.97 2.02 2.07 2.12 2.18 2.23 2.29 2.35 2.42 2.48 2.55 2.63 2.71 3.22 3.27 3.32 3.37 3.43 3.49 3.54 3.60 3.67 3.73 3.79 3.86 3.93 4.00 4.08 4.16 4.24 4.33 4.41 4.51 4.61 4.64 4.70 4.76 4.83 4.89 4.96 5.02 5.09 5.17 5.24 5.32 5.40 5.48 5.56 5.65 5.74 5.83 5.93 6.03 6.14 6.25 üì¢ Taille de l‚Äô√©chantillon Les tests d‚Äôind√©pendance sont tr√©s sensibles √† la taille des √©chantillons. Ici on divise par 100 pour avoir des effectifs faibles mais en conservant les r√©partitions. chisq.test(data/100) ## ## Pearson&#39;s Chi-squared test ## ## data: data/100 ## X-squared = 0.060504, df = 3, p-value = 0.9961 H0 : Variables ind√©pendantes si p-value &gt; 5% Ici on multiplie par 100 pour avoir des effectifs grands mais en conservant les r√©partitions chisq.test(data*100) ## ## Pearson&#39;s Chi-squared test ## ## data: data * 100 ## X-squared = 605.04, df = 3, p-value &lt; 2.2e-16 H1 : Variables non ind√©pendantes si p-value &lt; 5% 2.3 ANOVA 1 On effectue une analyse de variance pour mesurer l‚Äôind√©pendance entre une variable qualitative et une quantitative. Pour illustrer cela, on utilise le jeu de donn√©es Hotdogs üå≠. df &lt;- read.csv(file = &quot;./datasets/Hotdogs.csv&quot;, sep = &quot;;&quot;) Type Calories Sodium Beef 186 495 Beef 181 477 Beef 176 425 Beef 149 322 Beef 184 482 Beef 190 587 Beef 158 370 Beef 139 322 Beef 175 479 Beef 148 375 Beef 152 330 Beef 111 300 Beef 141 386 Beef 153 401 Beef 190 645 Beef 157 440 Beef 131 317 Beef 149 319 Beef 135 298 Beef 132 253 Meat 173 458 Meat 191 506 Meat 182 473 Meat 190 545 Meat 172 496 Meat 147 360 Meat 146 387 Meat 139 386 Meat 175 507 Meat 136 393 Meat 179 405 Meat 153 372 Meat 107 144 Meat 195 511 Meat 135 405 Meat 140 428 Meat 138 339 Poultry 129 430 Poultry 132 375 Poultry 102 396 Poultry 106 383 Poultry 94 387 Poultry 102 542 Poultry 87 359 Poultry 99 357 Poultry 107 528 Poultry 113 513 Poultry 135 426 Poultry 142 513 Poultry 86 358 Poultry 143 581 Poultry 152 588 Poultry 146 522 Poultry 144 545 On va tester l‚Äôind√©pendance entre la variable qualitative Type et la variable quantitatives Calories. boxplot(Calories ~ Type, data = df, horizontal = TRUE) Dans une ANOVA, on cherche √† d√©terminer si les moyennes des groupes sont significativement diff√©rentes. On pose donc : H0 : Les moyennes de chaque groupe sont √©gales si p-value &gt; 5% H1 : Les moyennes de chaque groupe ne sont pas toutes √©gales si p-value &lt; 5% Dans une ANOVA, on √©tudie la variance de chacun de ces groupes. Pour cela on utilise la fonction aov(). aov &lt;- aov(formula = Calories ~ Type, data = df) summary(aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Type 2 17692 8846 16.07 3.86e-06 *** ## Residuals 51 28067 550 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 H1 : Les moyennes de chaque groupe ne sont pas toutes √©gales Quand on dispose d‚Äôun petit √©chantillon, la pertinence de ce test repose sur la validation de plusieurs hypoth√®ses : l‚Äôind√©pendance entre les √©chantillons de chaque groupe l‚Äô√©galit√© des des variances que l‚Äôon peut verifier avec un test de Bartlett. la normal 2.3.1 L‚Äôind√©pendance L‚Äôind√©pendance est une des 3 conditions de validit√© d‚Äôune ANOVA. Seul le contexte de l‚Äô√©tude permet de s‚Äôassurer de l‚Äôind√©pendance entre les √©chantillons de chaque groupe (ici beef, poultry, chicken.) 2.3.2 L‚Äô√©galit√© des variances On parle aussi d‚Äôhomosc√©dasticit√©. C‚Äôest une des 3 conditions de validit√© d‚Äôune ANOVA. On cherche √† d√©montrer que les variances de chaque groupe sont √©gales. Dans un boxplot, l‚Äôamplitude des bo√Ætes traduit graphiquement l‚Äô√©galit√© des variances. boxplot(Calories ~ Type, data = df, horizontal = TRUE) Mais c‚Äôest le test de bartlett qui permet de tester si les variances sont significativement diff√©rentes ou non avec : H0 : Les variances de chaque groupe sont √©gales si p-value &gt; 5% H1 : Les variances de chaque groupe ne sont pas toutes √©gales &lt; 5% bartlett.test(Calories ~ Type, data = df) ## ## Bartlett test of homogeneity of variances ## ## data: Calories by Type ## Bartlett&#39;s K-squared = 0.26732, df = 2, p-value = 0.8749 H0 : Les variances de chaque groupe sont √©gales. La deuxi√®me condition pour effectuer une anova est valid√©e. 2.3.3 Normalit√© des r√©sidus C‚Äôest une des 3 conditions de validit√© d‚Äôune ANOVA. L‚Äôobjectif est de s‚Äôassurer que les r√©sidus suivent une loi normale afin de ne pas affirmer qu‚Äôil existe une diff√©rence de moyenne entre les groupes qui serait caus√©e par le hasard. Dans R, on utilise le test de Shapiro-Wilk pour tester la normalit√© des r√©sidus o√π : H0 : Les r√©sidus suivent une loi normale si p-value &gt; 5% H1 : Les r√©sidus ne suivent pas une loi normale si p-value &lt; 5% aov &lt;- aov(formula = Calories ~ Type, data = df) shapiro.test(aov$residuals) ## ## Shapiro-Wilk normality test ## ## data: aov$residuals ## W = 0.94199, p-value = 0.0113 H1 : Les r√©sidus ne suivent pas une loi normale 2.3.4 Calcul - Cas des variances √©gales a &lt;- seq(from = 1, to = 11, length.out = 9 ) b &lt;- seq(from = 31, to = 40, length.out = 9 ) c &lt;- seq(from = 51, to = 62, length.out = 9 ) df &lt;- data.frame(Valeur = c(a,b,c), Groupe = c(rep(&quot;A&quot;,9), rep(&quot;B&quot;,9), rep(&quot;C&quot;,9))) head(df) ## Valeur Groupe ## 1 1.00 A ## 2 2.25 A ## 3 3.50 A ## 4 4.75 A ## 5 6.00 A ## 6 7.25 A boxplot(Valeur ~ Groupe, data = df, col = 1:3, horizontal = TRUE) Comment calculer le tableau r√©captitulatif de l‚Äôanalyse de la variance : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Groupe 2 11585 5792 491 &lt;2e-16 *** ## Residuals 24 283 12 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Variance intra classes SCE_a &lt;- (a - mean(a))^2 SCE_b &lt;- (b - mean(b))^2 SCE_c &lt;- (c - mean(c))^2 intra &lt;- sum(SCE_a + SCE_b + SCE_c) intra ## [1] 283.125 Variance inter classes moyenne &lt;- mean(df$Valeur) moyenne_facteur &lt;- tapply(X = df$Valeur, INDEX = df$Groupe, FUN = mean) longueur_facteur &lt;- tapply(X = df$Valeur, INDEX = df$Groupe, FUN = length) inter &lt;- sum(longueur_facteur*((moyenne_facteur - moyenne)^2)) inter ## [1] 11584.5 Degr√© de libert√© n &lt;- nrow(df) p &lt;- length(levels(df$Groupe)) dof_inter &lt;- p - 1 dof_intra &lt;- n - p dof_inter ## [1] 2 dof_intra ## [1] 24 Calcul de la statistique de test de Fisher Stat_Fisher &lt;- (inter/dof_inter) / (intra/dof_intra) Stat_Fisher ## [1] 490.9987 On lit dans la table de Fisher pvalue &lt;- 1-pf(q = Stat_Fisher, df1 = dof_inter, df2 = dof_intra) pvalue ## [1] 0 R√©ciproque de la loi de Fisher pour retrouver la statistique de test. qf(p = 1-pvalue, df1 = dof_inter, df2 = dof_intra) ## [1] Inf 2.3.5 Calcul - Cas des variances in√©gales a &lt;- seq(from = 1, to = 40, length.out = 9 ) b &lt;- seq(from = 10, to = 30, length.out = 9 ) c &lt;- seq(from = 25, to = 30, length.out = 9 ) df &lt;- data.frame(Valeur = c(a,b,c), Groupe = c(rep(&quot;A&quot;,9), rep(&quot;B&quot;,9), rep(&quot;C&quot;,9))) head(df) ## Valeur Groupe ## 1 1.000 A ## 2 5.875 A ## 3 10.750 A ## 4 15.625 A ## 5 20.500 A ## 6 25.375 A boxplot(Valeur ~ Groupe, data = df, col = 1:3, horizontal = TRUE) Comment calculer le tableau r√©captitulatif de l‚Äôanalyse de la variance : ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Groupe 2 316.5 158.25 2.082 0.147 ## Residuals 24 1824.4 76.02 Variance intra classes SCE_a &lt;- (a - mean(a))^2 SCE_b &lt;- (b - mean(b))^2 SCE_c &lt;- (c - mean(c))^2 intra &lt;- sum(SCE_a + SCE_b + SCE_c) intra ## [1] 1824.375 Variance inter classes moyenne &lt;- mean(df$Valeur) moyenne_facteur &lt;- tapply(X = df$Valeur, INDEX = df$Groupe, FUN = mean) longueur_facteur &lt;- tapply(X = df$Valeur, INDEX = df$Groupe, FUN = length) inter &lt;- sum(longueur_facteur*((moyenne_facteur - moyenne)^2)) inter ## [1] 316.5 Degr√© de libert√© n &lt;- nrow(df) p &lt;- length(levels(df$Groupe)) dof_inter &lt;- p - 1 dof_intra &lt;- n - p dof_inter ## [1] 2 dof_intra ## [1] 24 Calcul de la statistique de test de Fisher Stat_Fisher &lt;- (inter/dof_inter) / (intra/dof_intra) Stat_Fisher ## [1] 2.081809 On lit dans la table de Fisher pvalue &lt;- 1-pf(q = Stat_Fisher, df1 = dof_inter, df2 = dof_intra) pvalue ## [1] 0.1466471 R√©ciproque de la loi de Fisher pour retrouver la statistique de test. qf(p = 1-pvalue, df1 = dof_inter, df2 = dof_intra) ## [1] 2.081809 2.4 ANOVA 2 "]
]
