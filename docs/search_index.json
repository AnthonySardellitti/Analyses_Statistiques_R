[
["independance.html", "Chapitre 2 Test ind√©pendance 2.1 Test de corr√©lation 2.2 Test du CHI¬≤ 2.3 ANOVA", " Chapitre 2 Test ind√©pendance Les tests d‚Äôind√©pendances permettent de d√©finir s‚Äôil existe un lien entre deux variables. Il existe diff√©rent test d‚Äôind√©pence, en voici quelques exemples : Test ind√©pendance entre deux variables quantitatives / Test de corr√©lation Pearson Test d‚Äôind√©pendance entre deux variables qualitatives / Test du Chi¬≤ Test d‚Äôind√©pendance entre une variable qualitative et une quantitative / Test de Fisher avec l‚Äôanalyse de la variance (ANOVA) 2.1 Test de corr√©lation L‚Äôint√©r√™t des tests de corr√©lation est d‚Äôapporter plus de pertinence et fiabilit√© aux coefficients de corr√©lation. Il existe diff√©rents test de corr√©lation, nous utilisons celui de Pearson. On travaille avec le jeu de donn√©es fromage üßÄ df &lt;- read.csv(file = &quot;./datasets/fromage.txt&quot;, sep = &quot;\\t&quot;, row.names = 1) plot(df) cor(df, method = &quot;pearson&quot;) ## calories sodium calcium lipides retinol ## calories 1.00000000 0.447224073 0.433400087 0.98363377 -0.04288511 ## sodium 0.44722407 1.000000000 0.005958892 0.48334434 0.14432755 ## calcium 0.43340009 0.005958892 1.000000000 0.34113187 -0.28954677 ## lipides 0.98363377 0.483344342 0.341131874 1.00000000 -0.01835291 ## retinol -0.04288511 0.144327552 -0.289546767 -0.01835291 1.00000000 ## folates -0.32109786 0.136561416 -0.637502539 -0.27654282 0.51726643 ## proteines 0.88537159 0.275997906 0.610704864 0.80930257 -0.03917254 ## cholesterol 0.96192339 0.332993972 0.428446215 0.95544387 -0.08697243 ## magnesium 0.74544320 0.031112805 0.710480424 0.68986008 -0.09980065 ## folates proteines cholesterol magnesium ## calories -0.3210979 0.88537159 0.96192339 0.74544320 ## sodium 0.1365614 0.27599791 0.33299397 0.03111281 ## calcium -0.6375025 0.61070486 0.42844622 0.71048042 ## lipides -0.2765428 0.80930257 0.95544387 0.68986008 ## retinol 0.5172664 -0.03917254 -0.08697243 -0.09980065 ## folates 1.0000000 -0.35287125 -0.36634610 -0.44706739 ## proteines -0.3528712 1.00000000 0.81713835 0.78777726 ## cholesterol -0.3663461 0.81713835 1.00000000 0.75245338 ## magnesium -0.4470674 0.78777726 0.75245338 1.00000000 library(corrplot) ## Warning: package &#39;corrplot&#39; was built under R version 3.5.3 ## corrplot 0.84 loaded corrplot(cor(df, method = &quot;pearson&quot;)) On pose les hypoth√®ses de d√©part H0 : Variables ind√©pendantes si p-value &gt; 5% H1 : Variables non ind√©pendantes si p-value &lt; 5% 2.1.1 Lipide vs Magnesium La premi√®re sortie correspond au coefficient de corr√©lation, la seconde √† la p-value (ou probabilit√© critique) cor(x = df$lipides, y = df$magnesium) ## [1] 0.6898601 cor.test(x = df$lipides, y = df$magnesium) ## ## Pearson&#39;s product-moment correlation ## ## data: df$lipides and df$magnesium ## t = 4.9515, df = 27, p-value = 3.469e-05 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4327766 0.8431785 ## sample estimates: ## cor ## 0.6898601 H1 : Variables non ind√©pendantes si p-value &lt; 5% 2.1.2 Sodium vs Retinol cor.test(x = df$sodium, y = df$retinol) ## ## Pearson&#39;s product-moment correlation ## ## data: df$sodium and df$retinol ## t = 0.75788, df = 27, p-value = 0.4551 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.2345870 0.4851693 ## sample estimates: ## cor ## 0.1443276 H0 : Variables ind√©pendantes si p-value &gt; 5% Si on veut rejeter H0 et prendre H1, j‚Äôai 45,5% de chance de me tromper Les tests statistiques sont tr√©s sensibles √† la taille de l‚Äô√©chantillon. Un coefficient de corr√©lation de 0.14 n‚Äôaura pas la m√™me significativit√© sur un √©chantillon de 29 fromages qu‚Äôun √©chantillon de 319 fromages avec le m√™me coefficient de corr√©lation. On construit un dataframe en dupliquant le nombre de lignes sodium &lt;- rep(df$sodium,times = 10) retinol &lt;- rep(df$retinol,times = 10) nom &lt;- rep(rownames(df),times = 10) df_10 &lt;- data.frame(nom,sodium,retinol) Chaque fromage appara√Æt plusieurs fois, on a augment√© la taille de l‚Äô√©chantillon table(df_10$nom) ## ## Babybel Beaufort Bleu ## 10 10 10 ## Camembert Cantal CarredelEst ## 10 10 10 ## Chabichou Chaource Cheddar ## 10 10 10 ## Comte Coulomniers Edam ## 10 10 10 ## Emmental Fr.chevrepatemolle Fr.fondu.45 ## 10 10 10 ## Fr.frais20nat. Fr.frais40nat. Maroilles ## 10 10 10 ## Morbier Parmesan Petitsuisse40 ## 10 10 10 ## PontlEveque Pyrenees Reblochon ## 10 10 10 ## Rocquefort SaintPaulin Tome ## 10 10 10 ## Vacherin Yaourtlaitent.nat. ## 10 10 On effectue un autre test de corr√©lation avec les m√™mes variables sur l‚Äô√©chantillon plus grand. cor.test(x = df_10$sodium, y = df_10$retinol) ## ## Pearson&#39;s product-moment correlation ## ## data: df_10$sodium and df_10$retinol ## t = 2.4752, df = 288, p-value = 0.01389 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.0296407 0.2552637 ## sample estimates: ## cor ## 0.1443276 H1 : Variables non ind√©pendantes si p-value &lt; 5% On obtient logiquement le m√™me coefficient de corr√©lation, mais en revanche, cette fois si la p-value est proche de 0. 2.1.3 Matrice des p-values On effectue un test de corr√©lation sur chaque variable 2 √† 2 en isolant uniquement la p-value get_pvalue &lt;- function(x,y){ p &lt;- cor.test(df[,x],df[,y])$p.value return(p) } colonne &lt;- colnames(df) ligne &lt;- colnames(df) df_pvalues &lt;- outer(X = colonne, Y = ligne, FUN = Vectorize(get_pvalue)) colnames(df_pvalues) &lt;- colnames(df) rownames(df_pvalues) &lt;- colnames(df) On affiche la matrice des corr√©lations avec un gradiant de couleur corrplot(df_pvalues, cl.lim=c(0,1), method=&quot;number&quot;, type=&quot;upper&quot;, col=colorRampPalette(c(&quot;white&quot;,&quot;white&quot;,&quot;red&quot;,&quot;yellow&quot;,&quot;green&quot;))(20)) 2.1.4 Cas de relation non lin√©aire Plus d‚Äôinfos ici Cas d‚Äôune relation non-lin√©aire et non-monotone x &lt;- -10:10 y &lt;- x^2 + rnorm(n = length(x)) plot(x,y) cor.test(x, y, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 0.045472, df = 19, p-value = 0.9642 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.4231609 0.4401362 ## sample estimates: ## cor ## 0.01043145 cor.test(x, y, method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: x and y ## S = 1466, p-value = 0.8368 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.04805195 cor.test(x, y, method = &quot;kendall&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: x and y ## T = 107, p-value = 0.9287 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.01904762 2.2 Test du CHI¬≤ L‚Äôint√©r√™t du test du Khi¬≤ est de mesurer l‚Äôind√©pendance entre deux variables qualitatives √† partir du tableau de contigence. H0 : Variables ind√©pendantes si p-value &gt; 5% H1 : Variables non ind√©pendantes si p-value &lt; 5% 2.2.1 Titanic On travaille sur le jeu de donn√©es Titanic üßä‚õ¥ df &lt;- read.csv(file = &quot;./datasets/Titanic.csv&quot;, row.names = 1) df_count &lt;- table(df$Survived, df$PClass) resultat &lt;-chisq.test(df$Survived,df$PClass) resultat ## ## Pearson&#39;s Chi-squared test ## ## data: df$Survived and df$PClass ## X-squared = 172.3, df = 2, p-value &lt; 2.2e-16 H1 : Variables non ind√©pendantes si p-value &lt; 5% attributes(resultat) ## $names ## [1] &quot;statistic&quot; &quot;parameter&quot; &quot;p.value&quot; &quot;method&quot; &quot;data.name&quot; &quot;observed&quot; ## [7] &quot;expected&quot; &quot;residuals&quot; &quot;stdres&quot; ## ## $class ## [1] &quot;htest&quot; resultat$expected ## df$PClass ## df$Survived 1st 2nd 3rd ## 0 211.642 184.03656 467.3214 ## 1 110.358 95.96344 243.6786 2.2.2 Exemple du support data &lt;- matrix(rbind(c(693,886,534,153),c(597,696,448,95)),ncol=4) chisq.test(data) ## ## Pearson&#39;s Chi-squared test ## ## data: data ## X-squared = 6.0504, df = 3, p-value = 0.1092 H0 : Variables ind√©pendantes si p-value &gt; 5% Si on veut rejeter H0 et prendre H1, j‚Äôai 10,9% de chance de me tromper Lecture dans la table du Chi2 p &lt;- seq(0.80, 0.90, 0.005) dof &lt;- seq(1,3) chisq_table &lt;- outer(p, dof, function(x,y) qchisq(x,y)) chisq_table &lt;- t(chisq_table) colnames(chisq_table) &lt;- 1 - p rownames(chisq_table) &lt;- dof chisq_table &lt;- round(chisq_table,2) 0.2 0.195 0.19 0.185 0.18 0.175 0.17 0.165 0.16 0.155 0.15 0.145 0.14 0.135 0.13 0.125 0.12 0.115 0.11 0.105 0.1 1.64 1.68 1.72 1.76 1.80 1.84 1.88 1.93 1.97 2.02 2.07 2.12 2.18 2.23 2.29 2.35 2.42 2.48 2.55 2.63 2.71 3.22 3.27 3.32 3.37 3.43 3.49 3.54 3.60 3.67 3.73 3.79 3.86 3.93 4.00 4.08 4.16 4.24 4.33 4.41 4.51 4.61 4.64 4.70 4.76 4.83 4.89 4.96 5.02 5.09 5.17 5.24 5.32 5.40 5.48 5.56 5.65 5.74 5.83 5.93 6.03 6.14 6.25 üì¢ Taille de l‚Äô√©chantillon Les tests d‚Äôind√©pendance sont tr√©s sensibles √† la taille des √©chantillons. Ici on divise par 100 pour avoir des effectifs faibles mais en conservant les r√©partitions. chisq.test(data/100) ## ## Pearson&#39;s Chi-squared test ## ## data: data/100 ## X-squared = 0.060504, df = 3, p-value = 0.9961 H0 : Variables ind√©pendantes si p-value &gt; 5% Ici on multiplie par 100 pour avoir des effectifs grands mais en conservant les r√©partitions chisq.test(data*100) ## ## Pearson&#39;s Chi-squared test ## ## data: data * 100 ## X-squared = 605.04, df = 3, p-value &lt; 2.2e-16 H1 : Variables non ind√©pendantes si p-value &lt; 5% 2.3 ANOVA On effectue une analyse de variance pour mesurer l‚Äôind√©pendance entre une variable qualitative et une quantitative. La pertinence de ce test repose sur les hypoth√®ses d‚Äôind√©pendance de chaque groupe ainsi que l‚Äô√©galit√© des variances que l‚Äôon peut verifier avec un test de Bartlett. C‚Äôest avec un test de Fisher qu‚Äôon peut ensuite r√©aliser l‚ÄôANOVA. 2.3.1 Cas de variances √©gales entre chaque groupe a &lt;- seq(from = 1, to = 11, length.out = 9 ) b &lt;- seq(from = 31, to = 40, length.out = 9 ) c &lt;- seq(from = 51, to = 62, length.out = 9 ) df &lt;- data.frame(Valeur = c(a,b,c), Groupe = c(rep(&quot;A&quot;,9), rep(&quot;B&quot;,9), rep(&quot;C&quot;,9))) head(df) ## Valeur Groupe ## 1 1.00 A ## 2 2.25 A ## 3 3.50 A ## 4 4.75 A ## 5 6.00 A ## 6 7.25 A boxplot(Valeur ~ Groupe, data = df, col = 1:3, horizontal = TRUE) On s‚Äôinteresse au variance de chaque groupe tapply(X = df$Valeur, INDEX = df$Groupe,FUN = var) ## A B C ## 11.718750 9.492188 14.179688 Le test de bartlett permet de tester si les variances sont significativement diff√©rentes ou non H0 : Les variances de chaque groupe sont √©gales si p-value &gt; 5% H1 : Les variances de chaque groupe ne sont pas toutes √©gales &lt; 5% bartlett.test(Valeur ~ Groupe, data = df) ## ## Bartlett test of homogeneity of variances ## ## data: Valeur by Groupe ## Bartlett&#39;s K-squared = 0.30342, df = 2, p-value = 0.8592 H0 : Les variances de chaque groupe sont √©gales si p-value &gt; 5% On peut donc faire une ANOVA A travers l‚Äôanalyse de la variance on cherche √† d√©terminer si : H0 : Les moyennes de chaque groupe sont √©gales si p-value &gt; 5% H1 : Les moyennes de chaque groupe ne sont pas toutes √©gales &lt; 5% aov &lt;- aov(formula = Valeur ~ Groupe, data = df) summary(aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Groupe 2 11585 5792 491 &lt;2e-16 *** ## Residuals 24 283 12 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 H1 : Les moyennes de chaque groupe ne sont pas toutes √©gales &lt; 5% Comment calculer le tableau : Calcul de la variance intra classes SCE_a &lt;- (a - mean(a))^2 SCE_b &lt;- (b - mean(b))^2 SCE_c &lt;- (c - mean(c))^2 within&lt;- sum(SCE_a + SCE_b + SCE_c) within ## [1] 283.125 Calcul de la variance inter classes moyenne &lt;- mean(df$Valeur) moyenne_facteur &lt;- tapply(X = df$Valeur, INDEX = df$Groupe, FUN = mean) longueur_facteur &lt;- tapply(X = df$Valeur, INDEX = df$Groupe, FUN = length) between &lt;- sum(longueur_facteur*((moyenne_facteur - moyenne)^2)) between ## [1] 11584.5 "]
]
